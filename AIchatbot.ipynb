{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinesh1213/PROJECT.PY/blob/main/AIchatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio>=4.44.0"
      ],
      "metadata": {
        "id": "B84bjlKpNdr0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install python-dotenv>=1.0.1\n",
        "!pip install openai>=1.40.0"
      ],
      "metadata": {
        "id": "ATA1aSX5Nvi7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from typing import List, Tuple\n",
        "import gradio as gr # Import gradio and alias as gr\n",
        "\n",
        "\n",
        "# Placeholder definitions - Replace with your actual implementations\n",
        "class Memory:\n",
        "    def __init__(self, max_turns: int):\n",
        "        self.max_turns = max_turns\n",
        "        self.turns = []\n",
        "\n",
        "    def add(self, role: str, content: str):\n",
        "        self.turns.append({'role': role, 'content': content})\n",
        "        if len(self.turns) > self.max_turns:\n",
        "            self.turns.pop(0) # Simple FIFO for exceeding max_turns\n",
        "\n",
        "    def as_messages(self):\n",
        "        return self.turns\n",
        "\n",
        "    def export_txt(self) -> str:\n",
        "        return \"\\n\".join((\"You: \" + t['content']) if t['role'] == \"user\" else (\"CalmCare: \" + t['content']) for t in self.turns)\n",
        "\n",
        "\n",
        "class LLM:\n",
        "    def __init__(self, api_key: str | None):\n",
        "        self.api_key = api_key\n",
        "        if not self.api_key:\n",
        "            print(\"OpenAI API key not found. Using rule-based fallback.\")\n",
        "\n",
        "    def generate(self, messages: list, user_message: str) -> str:\n",
        "        if not self.api_key:\n",
        "            # Simple rule-based fallback\n",
        "            if \"hello\" in user_message.lower():\n",
        "                return \"Hello there! How can I help you today?\"\n",
        "            elif \"how are you\" in user_message.lower():\n",
        "                return \"I am a chatbot, I don't have feelings, but I'm here to help you.\"\n",
        "            elif \"sad\" in user_message.lower() or \"unhappy\" in user_message.lower():\n",
        "                return \"I'm sorry to hear that you're feeling sad. Would you like to talk about it.\"\n",
        "            else:\n",
        "                return \"I'm here to listen if you'd like to share what's on your mind.\"\n",
        "        else:\n",
        "            # Replace with your actual OpenAI API call\n",
        "            return f\"LLM response to: {user_message}\" # Placeholder for actual LLM call\n",
        "\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "\n",
        "# ---- Gradio UI ----------------------------------------------------------------\n",
        "llm = LLM(OPENAI_API_KEY)\n",
        "\n",
        "\n",
        "def chat_fn(message: str, history: List[Tuple[str, str]]):\n",
        "    # Sync history → messages\n",
        "    mem_local = Memory(max_turns=24)\n",
        "    for u, a in history:\n",
        "        if u:\n",
        "            mem_local.add(\"user\", u)\n",
        "        if a:\n",
        "            mem_local.add(\"assistant\", a)\n",
        "    mem_local.add(\"user\", message or \"\")\n",
        "    reply = llm.generate(mem_local.as_messages(), message or \"\")\n",
        "    mem_local.add(\"assistant\", reply)\n",
        "    return reply\n",
        "\n",
        "\n",
        "with gr.Blocks(title=\"CalmCare – Mental Health Chatbot\", analytics_enabled=False) as demo:\n",
        "    gr.Markdown(\n",
        "        \"# CalmCare – Mental Health Chatbot\\n\"\n",
        "        \"*Supportive, empathetic conversation with a safety-first design.*\\n\\n\"\n",
        "        \"**Important:** This tool can’t provide diagnosis or replace professional care.\\n\"\n",
        "        \"If you may act on thoughts of harming yourself or someone else, contact your local emergency number.\"\n",
        "    )\n",
        "    gr.Markdown(\n",
        "        f\"Backend: {'OpenAI' if OPENAI_API_KEY else 'Rule‑based fallback'} — This is not medical advice.\"\n",
        "    )\n",
        "\n",
        "\n",
        "    chatbot = gr.Chatbot(label=\"CalmCare\", height=450, type=\"tuples\", show_copy_button=True)\n",
        "\n",
        "\n",
        "    with gr.Row():\n",
        "        msg = gr.Textbox(placeholder=\"Tell me what's on your mind…\", label=\"Your message\", autofocus=True)\n",
        "        with gr.Row():\n",
        "            send_btn = gr.Button(\"Send\", variant=\"primary\")\n",
        "            clear_btn = gr.Button(\"Clear Chat\")\n",
        "            export_btn = gr.Button(\"Export Conversation (.txt)\")\n",
        "\n",
        "\n",
        "    def _user_send(user_message, chat_history):\n",
        "        if not user_message:\n",
        "            return gr.update(), chat_history\n",
        "        response = chat_fn(user_message, chat_history)\n",
        "        chat_history = chat_history + [(user_message, response)]\n",
        "        return \"\", chat_history\n",
        "\n",
        "\n",
        "    send_btn.click(_user_send, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
        "    msg.submit(_user_send, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
        "\n",
        "    demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 705
        },
        "id": "-qCG1-ayOALv",
        "outputId": "48857af4-df1d-45de-9ecf-66d483a4e2e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API key not found. Using rule-based fallback.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2025061705.py:83: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(label=\"CalmCare\", height=450, type=\"tuples\", show_copy_button=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://713baf2b7f455e6fb5.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://713baf2b7f455e6fb5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j4ldYNBJwWwS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMsrqd9axo4oEyykh3Lj/Go",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}